{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveFiles = [x for x in os.listdir(\"movie_reviews/train/pos/\") if x.endswith(\".txt\")]\n",
    "negativeFiles = [x for x in os.listdir(\"movie_reviews/train/neg/\") if x.endswith(\".txt\")]\n",
    "testFiles = [x for x in os.listdir(\"movie_reviews/test/\") if x.endswith(\".txt\")]\n",
    "testScores = genfromtxt('test_rating.csv', delimiter=',', skip_header=1)\n",
    "positiveReviews, negativeReviews, testReviews = [], [], []\n",
    "for pfile in positiveFiles:\n",
    "    with open(\"movie_reviews/train/pos/\"+pfile, encoding=\"latin1\") as f:\n",
    "        positiveReviews.append(f.read())\n",
    "for nfile in negativeFiles:\n",
    "    with open(\"movie_reviews/train/neg/\"+nfile, encoding=\"latin1\") as f:\n",
    "        negativeReviews.append(f.read())\n",
    "for review in testScores:\n",
    "    with open(\"movie_reviews/test/\" + str(int(review[0])) + '.txt', encoding=\"latin1\") as f:\n",
    "        testReviews.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveReviews))\n",
    "print(len(negativeReviews))\n",
    "print(len(testReviews))\n",
    "print(len(testScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 1315.90 words (1001.387664)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "allTrainReviews = positiveReviews + negativeReviews\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in allTrainReviews]\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - pos\n",
    "# 2 - neg\n",
    "train_Y = [[1]] * len(positiveReviews) + [[0]] * len(negativeReviews)\n",
    "test_Y = []\n",
    "for item in my_data:\n",
    "    if int(item[1]) >= 7:\n",
    "        test_Y.append(1)\n",
    "    elif int(item[1]) <= 4:\n",
    "        test_Y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "allTrainReviews = list(map(lambda x: x.lower(), allTrainReviews))\n",
    "testFiles = list(map(lambda x: x.lower(), testFiles))\n",
    "\n",
    "train_X = allTrainReviews\n",
    "test_X = testFiles\n",
    "X = train_X + test_X\n",
    "train_Y = to_categorical(train_Y)\n",
    "test_Y = to_categorical(test_Y)\n",
    "\n",
    "max_words = 10000\n",
    "maxlen = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 2,323,856\n",
      "Trainable params: 2,323,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=maxlen))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      " - 16s - loss: 0.1071 - acc: 0.9637 - val_loss: 0.6972 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 15s - loss: 0.0465 - acc: 0.9875 - val_loss: 0.6979 - val_acc: 0.4999\n",
      "Epoch 3/10\n",
      " - 15s - loss: 0.0148 - acc: 0.9978 - val_loss: 0.7001 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 15s - loss: 0.0042 - acc: 0.9996 - val_loss: 0.7002 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 15s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7015 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 15s - loss: 4.3821e-04 - acc: 1.0000 - val_loss: 0.7045 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 15s - loss: 2.3716e-04 - acc: 1.0000 - val_loss: 0.7080 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 15s - loss: 1.2895e-04 - acc: 1.0000 - val_loss: 0.7153 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 15s - loss: 6.3834e-05 - acc: 1.0000 - val_loss: 0.7216 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 15s - loss: 3.4250e-05 - acc: 1.0000 - val_loss: 0.7320 - val_acc: 0.4999\n",
      "Accuracy: 49.99%\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, validation_data=(test_X, test_Y), epochs=10, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_X, test_Y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_words+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop\n",
    "\n",
    "def build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, dropout, learning_rate, multiple_fc, fc_units):\n",
    "    '''Build the Recurrent Neural Network'''\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "\n",
    "    with tf.name_scope('labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # Create the embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embedding = tf.Variable(tf.random_uniform((n_words, \n",
    "                                    embed_size), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    # Set the initial state\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                                        cell,         \n",
    "                                        embed,\n",
    "                                        initial_state=initial_state)    \n",
    "    \n",
    "    # Create the fully connected layers\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        \n",
    "        # Initialize the weights and biases\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        dense = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
    "                    num_outputs = fc_units,\n",
    "                    activation_fn = tf.sigmoid,\n",
    "                    weights_initializer = weights,\n",
    "                    biases_initializer = biases)\n",
    "        \n",
    "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "        \n",
    "        # Depending on the iteration, use a second fully connected layer\n",
    "        if multiple_fc == True:\n",
    "            dense = tf.contrib.layers.fully_connected(dense,\n",
    "                        num_outputs = fc_units,\n",
    "                        activation_fn = tf.sigmoid,\n",
    "                        weights_initializer = weights,\n",
    "                        biases_initializer = biases)\n",
    "            \n",
    "            dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "    \n",
    "    # Make the predictions\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(dense, \n",
    "                          num_outputs = 1, \n",
    "                          activation_fn=tf.sigmoid,\n",
    "                          weights_initializer = weights,\n",
    "                          biases_initializer = biases)\n",
    "        \n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "    \n",
    "    # Calculate the cost\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Train the model\n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Determine the accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), \n",
    "                                        tf.int32), \n",
    "                                        labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'labels', 'keep_prob','initial_state',        \n",
    "                    'final_state','accuracy', 'predictions', 'cost', \n",
    "                    'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "n_words = len(word_index)\n",
    "embed_size = 300\n",
    "batch_size = 250\n",
    "lstm_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "multiple_fc = False\n",
    "fc_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string):\n",
    "    '''Train the RNN'''\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Used to determine when to stop the training early\n",
    "        valid_loss_summary = []\n",
    "        \n",
    "        # Keep track of which batch iteration is being trained\n",
    "        iteration = 0\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./logs/3/train/{}'.format(log_string), sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter('./logs/3/valid/{}'.format(log_string))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(model.initial_state)\n",
    "            \n",
    "            # Record progress with each epoch\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            val_acc = []\n",
    "            val_loss = []\n",
    "\n",
    "            with tqdm(total=len(train_X)) as pbar:\n",
    "                for _, (x, y) in enumerate(get_batches(train_X,       \n",
    "                                               train_Y, \n",
    "                                               batch_size), 1):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.labels: y[:, None],\n",
    "                            model.keep_prob: dropout,\n",
    "                            model.initial_state: state}\n",
    "                    summary, loss, acc, state, _ = sess.run([model.merged, \n",
    "                                                  model.cost, \n",
    "                                                  model.accuracy, \n",
    "                                                  model.final_state, \n",
    "                                                  model.optimizer], \n",
    "                                                  feed_dict=feed)                \n",
    "                    \n",
    "                    # Record the loss and accuracy of each training batch\n",
    "                    \n",
    "                    train_loss.append(loss)\n",
    "                    train_acc.append(acc)\n",
    "                    \n",
    "                    # Record the progress of training\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    \n",
    "                    iteration += 1\n",
    "                    pbar.update(batch_size)\n",
    "            \n",
    "            # Average the training loss and accuracy of each epoch\n",
    "            avg_train_loss = np.mean(train_loss)\n",
    "            avg_train_acc = np.mean(train_acc) \n",
    "\n",
    "            val_state = sess.run(model.initial_state)\n",
    "            with tqdm(total=len(x_valid)) as pbar:\n",
    "                for x, y in get_batches(x_valid,y_valid,batch_size):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.labels: y[:, None],\n",
    "                            model.keep_prob: 1,\n",
    "                            model.initial_state: val_state}\n",
    "                    summary, batch_loss, batch_acc, val_state = sess.run([model.merged, \n",
    "                                           model.cost, \n",
    "                                           model.accuracy, \n",
    "                                           model.final_state], \n",
    "                                           feed_dict=feed)\n",
    "                    \n",
    "                    # Record the validation loss and accuracy of each epoch\n",
    "                    \n",
    "                    val_loss.append(batch_loss)\n",
    "                    val_acc.append(batch_acc)\n",
    "                    pbar.update(batch_size)\n",
    "            \n",
    "            # Average the validation loss and accuracy of each epoch\n",
    "            avg_valid_loss = np.mean(val_loss)    \n",
    "            avg_valid_acc = np.mean(val_acc)\n",
    "            valid_loss_summary.append(avg_valid_loss)\n",
    "            \n",
    "            # Record the validation data's progress\n",
    "            valid_writer.add_summary(summary, iteration)\n",
    "\n",
    "            # Print the progress of each epoch\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                  \"Train Loss: {:.3f}\".format(avg_train_loss),\n",
    "                  \"Train Acc: {:.3f}\".format(avg_train_acc),\n",
    "                  \"Valid Loss: {:.3f}\".format(avg_valid_loss),\n",
    "                  \"Valid Acc: {:.3f}\".format(avg_valid_acc))\n",
    "\n",
    "            # Stop training if the validation loss does not decrease after 3 epochs\n",
    "            \n",
    "            if avg_valid_loss > min(valid_loss_summary):\n",
    "                print(\"No Improvement.\")\n",
    "                stop_early += 1\n",
    "                if stop_early == 3:\n",
    "                    break   \n",
    "            \n",
    "            # Reset stop_early if the validation loss finds a new low\n",
    "            # Save a checkpoint of the model\n",
    "            else:\n",
    "                print(\"New Record!\")\n",
    "                stop_early = 0\n",
    "                checkpoint =\"./sentiment_{}.ckpt\".format(log_string)\n",
    "                saver.save(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    '''Create the batches for the training and validation data'''\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "def get_test_batches(x, batch_size):\n",
    "    '''Create the batches for the testing data'''\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lstm_size in [64,128]:\n",
    "    for multiple_fc in [True, False]:\n",
    "        for fc_units in [128, 256]:\n",
    "            log_string = 'ru={},fcl={},fcu={}'.format(lstm_size,\n",
    "                                                      multiple_fc,\n",
    "                                                      fc_units)\n",
    "            model = build_rnn(n_words = n_words, \n",
    "                              embed_size = embed_size,\n",
    "                              batch_size = batch_size,\n",
    "                              lstm_size = lstm_size,\n",
    "                              num_layers = num_layers,\n",
    "                              dropout = dropout,\n",
    "                              learning_rate = learning_rate,\n",
    "                              multiple_fc = multiple_fc,\n",
    "                              fc_units = fc_units)            \n",
    "            train(model, epochs, log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(lstm_size, multiple_fc, fc_units, checkpoint):\n",
    "    '''Predict the sentiment of the testing data'''\n",
    "    \n",
    "    # Record all of the predictions\n",
    "    all_preds = []\n",
    "\n",
    "    model = build_rnn(n_words = n_words, \n",
    "                      embed_size = embed_size,\n",
    "                      batch_size = batch_size,\n",
    "                      lstm_size = lstm_size,\n",
    "                      num_layers = num_layers,\n",
    "                      dropout = dropout,\n",
    "                      learning_rate = learning_rate,\n",
    "                      multiple_fc = multiple_fc,\n",
    "                      fc_units = fc_units) \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # Load the model\n",
    "        saver.restore(sess, checkpoint)\n",
    "        test_state = sess.run(model.initial_state)\n",
    "        for _, x in enumerate(get_test_batches(test_X, \n",
    "                                               batch_size), 1):\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: test_state}\n",
    "            predictions = sess.run(model.predictions,feed_dict=feed)\n",
    "            for pred in predictions:\n",
    "                all_preds.append(float(pred))\n",
    "                \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
